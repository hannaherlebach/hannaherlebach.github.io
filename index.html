<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Hannah Erlebach</title>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
  <main>
    <h1>Hannah Erlebach</h1>
    <p>DPhil in Machine Learning @ FLAIR, University of Oxford</p>
    <p>Oxford · UK</p>
    <nav>
      <a href="#about">About</a>
      <a href="#research">Research</a>
      <a href="cv.pdf">CV</a>
      <button id="theme-toggle" aria-label="Toggle dark mode">
        <svg id="moon-icon" xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="currentColor">
          <path d="M21 12.79A9 9 0 0111.21 3a7 7 0 100 14 9 9 0 009.79-4.21z"/>
        </svg>
      </button>
      
    </nav>

    <section id="about">
      <h2>About</h2>
      <p>I’m a first year DPhil student in machine learning at the Univeristy of Oxford, supervised by Jakob Foerster. Previously, I completed an MSc in Machine Learning at University College London, and a BA in Mathematics at the University of Cambridge. I am funded by the Cooperative AI PhD fellowship.
        <br><br>
        I'm broadly interested in exploring questions about <i>minds</i>: what they are, where they come from, and how they <i>could</i> be. I also care deeply about AI alignment and what it means for diverse minds to co-exist harmoniously.
        <br><br>
        You can contact me at hannah [dot] erlebach [at] gmail [dot] com.
    </p>
    </section>

    <section id="research">
      <h2>Research</h2>
        <p>I'm currently thinking about <i>goals</i>: what they are and how they arise from a goal-less universe. Ultimately, I think that reward specification is an inescapable problem, and I'm curious about whether and how it's possible to induce interesting behaviour outside of the reward-maximisation paradigm.<br><br> My previous research has focused on cooperation in language models and multi-agent reinforcement learning settings.</p>
      <ul>
        <li><strong>DUA: Discovering Universal Attacks Using Foundation Models</strong>. <i>Master's thesis for UCL MSc in Machine Learning, 2025</i>.</li>
        <li><strong>Mitigating Goal Misgeneralisation via Minimax Regret</strong>. Karim Abdel Sadek, Matthew Farrugia-Roberts, Usman Anwar, <strong>Hannah Erlebach</strong>, Christian Schroeder de Witt, David Krueger and Michael Dennis. Published in <i>Reinforcement Learning Conference 2025.</i></li>
        <li><strong>RACCOON: Regret-based Adaptive Curricula for Cooperation</strong>. <strong>Hannah Erlebach</strong> and Jonathan Cook. Published in <i>CoCoMARL workshop at Reinforcement Learning Conference 2024.</i> </li>
        <li><strong>Welfare Diplomacy: Benchmarking Language Model Cooperation</strong>. Gabriel Mukobi, <strong>Hannah Erlebach</strong>, Niklas Lauffer, Lewis Hammond, Alan Chan and Jesse Clifton. Published in <i>SoLaR workshop at NeurIPS 2023.</i> [<a href="https://arxiv.org/abs/2310.08901">arXiv</a>]</li>
      </ul>
    </section>
  </main>
  <script>
    const toggle = document.getElementById('theme-toggle');
    const body = document.body;
  
    // On load: check saved preference
    if (localStorage.getItem('theme') === 'dark') {
      body.classList.add('dark-mode');
    }
  
    toggle.addEventListener('click', () => {
      const isDark = body.classList.toggle('dark-mode');
      localStorage.setItem('theme', isDark ? 'dark' : 'light');
    });
  </script>  
</body>
</html>
